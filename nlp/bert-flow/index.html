<!DOCTYPE html>
<html lang="en-US" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-beta.51" />
    <meta name="theme" content="VuePress Theme Hope" />
    <meta property="og:url" content="https://lileicc.github.io/blog/nlp/bert-flow/"><meta property="og:site_name" content="MLNLP Blog"><meta property="og:title" content="What is the problem with BERT embeddings and how to fix them?"><meta property="og:type" content="article"><meta property="og:image" content="https://lileicc.github.io/blog/"><meta property="og:updated_time" content="2022-09-13T03:45:15.000Z"><meta property="og:locale" content="en-US"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image:alt" content="What is the problem with BERT embeddings and how to fix them?"><meta property="article:author" content="Bohan Li"><meta property="article:tag" content="Pre-training"><meta property="article:tag" content="BERT"><meta property="article:tag" content="Embedding"><meta property="article:published_time" content="2020-11-04T00:00:00.000Z"><meta property="article:modified_time" content="2022-09-13T03:45:15.000Z"><title>What is the problem with BERT embeddings and how to fix them? | MLNLP Blog</title><meta name="description" content="A Blog for Machine Learning, Natural Language Processing, and Data Mining">
    <style>
      :root {
        --bg-color: #fff;
      }

      html[data-theme="dark"] {
        --bg-color: #1d2025;
      }

      html,
      body {
        background-color: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.querySelector("html").setAttribute("data-theme", "dark");
      }
    </script>
    <link rel="stylesheet" href="/blog/assets/style.758fbe27.css">
    <link rel="modulepreload" href="/blog/assets/app.4a645708.js"><link rel="modulepreload" href="/blog/assets/index.html.136b3991.js"><link rel="modulepreload" href="/blog/assets/_plugin-vue_export-helper.cdc0426e.js"><link rel="modulepreload" href="/blog/assets/index.html.e207b34f.js"><link rel="prefetch" href="/blog/assets/index.html.48d72793.js"><link rel="prefetch" href="/blog/assets/index.html.a49e14cc.js"><link rel="prefetch" href="/blog/assets/index.html.84f09450.js"><link rel="prefetch" href="/blog/assets/index.html.7513d19a.js"><link rel="prefetch" href="/blog/assets/index.html.948f6ba7.js"><link rel="prefetch" href="/blog/assets/index.html.4e8ce27b.js"><link rel="prefetch" href="/blog/assets/index.html.cc204d40.js"><link rel="prefetch" href="/blog/assets/index.html.9d3d23de.js"><link rel="prefetch" href="/blog/assets/index.html.6fe109c4.js"><link rel="prefetch" href="/blog/assets/index.html.f2001ec9.js"><link rel="prefetch" href="/blog/assets/index.html.f13e23f3.js"><link rel="prefetch" href="/blog/assets/index.html.36a209ea.js"><link rel="prefetch" href="/blog/assets/index.html.e51071bb.js"><link rel="prefetch" href="/blog/assets/index.html.3b54c059.js"><link rel="prefetch" href="/blog/assets/index.html.9bc33622.js"><link rel="prefetch" href="/blog/assets/index.html.1b2f4c5b.js"><link rel="prefetch" href="/blog/assets/index.html.2693264c.js"><link rel="prefetch" href="/blog/assets/index.html.371141f3.js"><link rel="prefetch" href="/blog/assets/index.html.e3b8fd71.js"><link rel="prefetch" href="/blog/assets/index.html.3cc17b99.js"><link rel="prefetch" href="/blog/assets/index.html.d6d3e578.js"><link rel="prefetch" href="/blog/assets/index.html.9a21b56f.js"><link rel="prefetch" href="/blog/assets/index.html.1f3a298a.js"><link rel="prefetch" href="/blog/assets/404.html.144ea56c.js"><link rel="prefetch" href="/blog/assets/index.html.3a32b4c3.js"><link rel="prefetch" href="/blog/assets/index.html.3eaba85c.js"><link rel="prefetch" href="/blog/assets/index.html.9c8e527a.js"><link rel="prefetch" href="/blog/assets/index.html.949c6932.js"><link rel="prefetch" href="/blog/assets/index.html.9369be8b.js"><link rel="prefetch" href="/blog/assets/index.html.5ecb22f0.js"><link rel="prefetch" href="/blog/assets/index.html.0a31196e.js"><link rel="prefetch" href="/blog/assets/index.html.1b68751c.js"><link rel="prefetch" href="/blog/assets/index.html.85fce0bc.js"><link rel="prefetch" href="/blog/assets/index.html.7d865fc8.js"><link rel="prefetch" href="/blog/assets/index.html.73acbfb4.js"><link rel="prefetch" href="/blog/assets/index.html.7e45f7bf.js"><link rel="prefetch" href="/blog/assets/index.html.620dcedf.js"><link rel="prefetch" href="/blog/assets/index.html.83d0711e.js"><link rel="prefetch" href="/blog/assets/index.html.b4a0aa08.js"><link rel="prefetch" href="/blog/assets/index.html.7adb01cb.js"><link rel="prefetch" href="/blog/assets/index.html.f005e561.js"><link rel="prefetch" href="/blog/assets/index.html.7fb0dc28.js"><link rel="prefetch" href="/blog/assets/index.html.71f8befa.js"><link rel="prefetch" href="/blog/assets/index.html.92c68d7e.js"><link rel="prefetch" href="/blog/assets/index.html.055d7455.js"><link rel="prefetch" href="/blog/assets/index.html.df6f3f04.js"><link rel="prefetch" href="/blog/assets/index.html.00dbedea.js"><link rel="prefetch" href="/blog/assets/index.html.603bf394.js"><link rel="prefetch" href="/blog/assets/index.html.cd4b9e10.js"><link rel="prefetch" href="/blog/assets/index.html.be0f419f.js"><link rel="prefetch" href="/blog/assets/index.html.bc51d0ae.js"><link rel="prefetch" href="/blog/assets/index.html.b79794f0.js"><link rel="prefetch" href="/blog/assets/index.html.eb8e4cee.js"><link rel="prefetch" href="/blog/assets/index.html.5c842c2c.js"><link rel="prefetch" href="/blog/assets/index.html.a95edb3c.js"><link rel="prefetch" href="/blog/assets/index.html.13db0b56.js"><link rel="prefetch" href="/blog/assets/index.html.bad01ddf.js"><link rel="prefetch" href="/blog/assets/index.html.ed77c4d1.js"><link rel="prefetch" href="/blog/assets/index.html.a3514a8d.js"><link rel="prefetch" href="/blog/assets/index.html.65bca0e5.js"><link rel="prefetch" href="/blog/assets/index.html.5bb552c5.js"><link rel="prefetch" href="/blog/assets/index.html.6eb9e54e.js"><link rel="prefetch" href="/blog/assets/index.html.62d61c88.js"><link rel="prefetch" href="/blog/assets/index.html.1362a299.js"><link rel="prefetch" href="/blog/assets/index.html.68f9bc07.js"><link rel="prefetch" href="/blog/assets/index.html.0272cd41.js"><link rel="prefetch" href="/blog/assets/index.html.3b47eb32.js"><link rel="prefetch" href="/blog/assets/index.html.d8dad1e7.js"><link rel="prefetch" href="/blog/assets/index.html.f01de255.js"><link rel="prefetch" href="/blog/assets/index.html.415c7585.js"><link rel="prefetch" href="/blog/assets/index.html.c876a430.js"><link rel="prefetch" href="/blog/assets/index.html.1b3b1bb2.js"><link rel="prefetch" href="/blog/assets/index.html.d1ba8a2a.js"><link rel="prefetch" href="/blog/assets/index.html.ba6b6066.js"><link rel="prefetch" href="/blog/assets/index.html.59861a75.js"><link rel="prefetch" href="/blog/assets/index.html.37e44878.js"><link rel="prefetch" href="/blog/assets/index.html.f61ca0f7.js"><link rel="prefetch" href="/blog/assets/index.html.8fd57ead.js"><link rel="prefetch" href="/blog/assets/index.html.1d75e0d7.js"><link rel="prefetch" href="/blog/assets/index.html.67b586be.js"><link rel="prefetch" href="/blog/assets/index.html.6b468b17.js"><link rel="prefetch" href="/blog/assets/index.html.ec88d877.js"><link rel="prefetch" href="/blog/assets/index.html.b9fddecd.js"><link rel="prefetch" href="/blog/assets/index.html.9f212efc.js"><link rel="prefetch" href="/blog/assets/index.html.edab7849.js"><link rel="prefetch" href="/blog/assets/index.html.3b1c2171.js"><link rel="prefetch" href="/blog/assets/index.html.a3ac0506.js"><link rel="prefetch" href="/blog/assets/index.html.8171ef25.js"><link rel="prefetch" href="/blog/assets/index.html.1e91f5fa.js"><link rel="prefetch" href="/blog/assets/index.html.3e1e2bfe.js"><link rel="prefetch" href="/blog/assets/index.html.2567ea47.js"><link rel="prefetch" href="/blog/assets/index.html.34179bc0.js"><link rel="prefetch" href="/blog/assets/index.html.07dd5142.js"><link rel="prefetch" href="/blog/assets/index.html.6e207c32.js"><link rel="prefetch" href="/blog/assets/index.html.e66e9084.js"><link rel="prefetch" href="/blog/assets/index.html.f68d02e9.js"><link rel="prefetch" href="/blog/assets/index.html.0170af13.js"><link rel="prefetch" href="/blog/assets/index.html.a0fad824.js"><link rel="prefetch" href="/blog/assets/404.html.bb5ce50d.js"><link rel="prefetch" href="/blog/assets/index.html.d25d3e45.js"><link rel="prefetch" href="/blog/assets/index.html.3c7083fc.js"><link rel="prefetch" href="/blog/assets/index.html.311c6849.js"><link rel="prefetch" href="/blog/assets/index.html.b97be9de.js"><link rel="prefetch" href="/blog/assets/index.html.1404e446.js"><link rel="prefetch" href="/blog/assets/index.html.478e9de3.js"><link rel="prefetch" href="/blog/assets/index.html.fad58c0c.js"><link rel="prefetch" href="/blog/assets/index.html.b5230444.js"><link rel="prefetch" href="/blog/assets/index.html.e59bc402.js"><link rel="prefetch" href="/blog/assets/index.html.a8bd35ae.js"><link rel="prefetch" href="/blog/assets/index.html.55416fe3.js"><link rel="prefetch" href="/blog/assets/index.html.dee446ce.js"><link rel="prefetch" href="/blog/assets/index.html.b1e6a269.js"><link rel="prefetch" href="/blog/assets/index.html.baf64357.js"><link rel="prefetch" href="/blog/assets/index.html.8350ec77.js"><link rel="prefetch" href="/blog/assets/index.html.bd651257.js"><link rel="prefetch" href="/blog/assets/index.html.df57d3ef.js"><link rel="prefetch" href="/blog/assets/index.html.a04b4e88.js"><link rel="prefetch" href="/blog/assets/index.html.5b3b35f7.js"><link rel="prefetch" href="/blog/assets/index.html.bf11daf8.js"><link rel="prefetch" href="/blog/assets/index.html.5cd8c87a.js"><link rel="prefetch" href="/blog/assets/index.html.0ea67a10.js"><link rel="prefetch" href="/blog/assets/index.html.f8ce58db.js"><link rel="prefetch" href="/blog/assets/index.html.b5a08197.js"><link rel="prefetch" href="/blog/assets/index.html.4c29b9a7.js"><link rel="prefetch" href="/blog/assets/index.html.5e9f59d7.js"><link rel="prefetch" href="/blog/assets/index.html.1b716da2.js"><link rel="prefetch" href="/blog/assets/index.html.0799ac9b.js"><link rel="prefetch" href="/blog/assets/index.html.7638ba74.js"><link rel="prefetch" href="/blog/assets/index.html.0521c2e3.js"><link rel="prefetch" href="/blog/assets/index.html.ac8ae03e.js"><link rel="prefetch" href="/blog/assets/index.html.dff0760b.js"><link rel="prefetch" href="/blog/assets/index.html.87a64487.js"><link rel="prefetch" href="/blog/assets/index.html.20ae52d7.js"><link rel="prefetch" href="/blog/assets/index.html.c2cf0552.js"><link rel="prefetch" href="/blog/assets/index.html.daeb6ccb.js"><link rel="prefetch" href="/blog/assets/index.html.d662a319.js"><link rel="prefetch" href="/blog/assets/index.html.ed066fbd.js"><link rel="prefetch" href="/blog/assets/index.html.c34d89d0.js"><link rel="prefetch" href="/blog/assets/index.html.4bf58d23.js"><link rel="prefetch" href="/blog/assets/index.html.6b48961d.js"><link rel="prefetch" href="/blog/assets/index.html.250aedc4.js"><link rel="prefetch" href="/blog/assets/index.html.2fb73d6d.js"><link rel="prefetch" href="/blog/assets/index.html.71433153.js"><link rel="prefetch" href="/blog/assets/index.html.c21cb6c2.js"><link rel="prefetch" href="/blog/assets/index.html.e515aa55.js"><link rel="prefetch" href="/blog/assets/index.html.0f5a3610.js"><link rel="prefetch" href="/blog/assets/index.html.7644a494.js"><link rel="prefetch" href="/blog/assets/index.html.d3b11d9c.js"><link rel="prefetch" href="/blog/assets/index.html.f19c87f9.js"><link rel="prefetch" href="/blog/assets/index.html.29ef0bb3.js"><link rel="prefetch" href="/blog/assets/giscus.15440425.js"><link rel="prefetch" href="/blog/assets/highlight.esm.d982e650.js"><link rel="prefetch" href="/blog/assets/markdown.esm.832a189d.js"><link rel="prefetch" href="/blog/assets/math.esm.a3f84b6f.js"><link rel="prefetch" href="/blog/assets/notes.esm.3c361cb7.js"><link rel="prefetch" href="/blog/assets/reveal.esm.b96f05d8.js"><link rel="prefetch" href="/blog/assets/search.esm.80da4a02.js"><link rel="prefetch" href="/blog/assets/zoom.esm.8514a202.js"><link rel="prefetch" href="/blog/assets/photoswipe.esm.382b1873.js">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="skip-link sr-only">Skip to content</a><!--]--><div class="theme-container no-sidebar has-toc"><!--[--><!--[--><header class="navbar"><div class="navbar-left"><button class="toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!----><a href="/blog/" class="brand"><img class="logo" src="/blog/logo.svg" alt="MLNLP Blog"><!----><span class="site-name hide-in-pad">MLNLP Blog</span></a><!----></div><div class="navbar-center"><!----><nav class="nav-links"><div class="nav-item hide-in-mobile"><a href="/blog/" class="nav-link" aria-label="Blog Home"><span class="icon iconfont icon-home"></span>Blog Home<!----></a></div><div class="nav-item hide-in-mobile"><a href="/blog/category/" class="nav-link" aria-label="Category"><span class="icon iconfont icon-categoryselected"></span>Category<!----></a></div><div class="nav-item hide-in-mobile"><a href="/blog/tag/" class="nav-link" aria-label="Tags"><span class="icon iconfont icon-tag"></span>Tags<!----></a></div><div class="nav-item hide-in-mobile"><a href="/blog/timeline/" class="nav-link" aria-label="Timeline"><span class="icon iconfont icon-time"></span>Timeline<!----></a></div></nav><!----></div><div class="navbar-right"><!----><!----><div class="nav-item"><a class="repo-link" href="https://github.com/lileicc/blog" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="nav-item hide-in-mobile"><button id="appearance-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="light icon" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><form class="search-box" role="search"><input type="search" autocomplete="off" spellcheck="false" value><!----></form><!----><button class="toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span class="button-container"><span class="button-top"></span><span class="button-middle"></span><span class="button-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow left"></span></div><aside class="sidebar"><!--[--><!----><!--]--><ul class="sidebar-links"></ul><!--[--><!----><!--]--></aside><!--[--><main class="page" id="main-content"><!--[--><!----><nav class="breadcrumb disable"></nav><div class="page-title"><h1><!---->What is the problem with BERT embeddings and how to fix them?</h1><div class="page-info"><span class="author-info" aria-label="Author🖊" data-balloon-pos="down" localizeddate="November 4, 2020" isoriginal="false" pageview="false"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><span class="author-item">Bohan Li</span></span><span property="author" content="Bohan Li"></span></span><!----><span class="date-info" aria-label="Writing Date📅" data-balloon-pos="down" isoriginal="false" pageview="false"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span>November 4, 2020</span><meta property="datePublished" content="2020-11-04T00:00:00.000Z"></span><span class="category-info" aria-label="Category🌈" data-balloon-pos="down" localizeddate="November 4, 2020" isoriginal="false" pageview="false"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="category icon"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><ul class="categories-wrapper"><li class="category category8 clickable" role="navigation">NLP</li><meta property="articleSection" content="NLP"></ul></span><span aria-label="Tag🏷" data-balloon-pos="down" localizeddate="November 4, 2020" isoriginal="false" pageview="false"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="tag icon"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><ul class="tags-wrapper"><li class="tag tag6 clickable" role="navigation">Pre-training</li><li class="tag tag3 clickable" role="navigation">BERT</li><li class="tag tag4 clickable" role="navigation">Embedding</li></ul><meta property="keywords" content="Pre-training,BERT,Embedding"></span><span class="reading-time-info" aria-label="Reading Time⌛" data-balloon-pos="down" localizeddate="November 4, 2020" isoriginal="false" pageview="false"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>About 4 min</span><meta property="timeRequired" content="PT4M"></span></div><hr></div><div class="toc-place-holder"><aside id="toc"><div class="toc-header">On This Page</div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a aria-current="page" href="/blog/nlp/bert-flow/#background" class="router-link-active router-link-exact-active toc-link level2">Background</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/blog/nlp/bert-flow/#major-questions" class="router-link-active router-link-exact-active toc-link level2">Major Questions</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/blog/nlp/bert-flow/#our-findings" class="router-link-active router-link-exact-active toc-link level2">Our Findings</a></li><ul class="toc-list"><!--[--><li class="toc-item"><a aria-current="page" href="/blog/nlp/bert-flow/#the-anisotropic-embedding-space-of-bert" class="router-link-active router-link-exact-active toc-link level3">The Anisotropic Embedding Space of BERT</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/blog/nlp/bert-flow/#word-frequency-biases-the-embedding-space" class="router-link-active router-link-exact-active toc-link level3">Word Frequency Biases the Embedding Space</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/blog/nlp/bert-flow/#low-frequency-words-disperse-sparsely" class="router-link-active router-link-exact-active toc-link level3">Low-Frequency Words Disperse Sparsely</a></li><!----><!--]--></ul><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/blog/nlp/bert-flow/#proposed-method-bert-flow" class="router-link-active router-link-exact-active toc-link level2">Proposed Method: BERT-flow</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/blog/nlp/bert-flow/#experiments" class="router-link-active router-link-exact-active toc-link level2">Experiments</a></li><ul class="toc-list"><!--[--><li class="toc-item"><a aria-current="page" href="/blog/nlp/bert-flow/#results-w-o-nli-supervision" class="router-link-active router-link-exact-active toc-link level3">Results w/o NLI Supervision</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/blog/nlp/bert-flow/#results-w-nli-supervision" class="router-link-active router-link-exact-active toc-link level3">Results w/ NLI Supervision</a></li><!----><!--]--></ul><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/blog/nlp/bert-flow/#conclusion" class="router-link-active router-link-exact-active toc-link level2">Conclusion</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/blog/nlp/bert-flow/#reference" class="router-link-active router-link-exact-active toc-link level2">Reference</a></li><!----><!--]--></ul></div></aside></div><!----><div class="theme-hope-content"><p>This blog presents an easy fix to the sentence embeddings learned by pre-trained language models. It is based on the paper: On the Sentence Embeddings from Pre-trained Language Models by Li et al EMNLP 2020.</p><!-- more --><p>Paper: <a href="https://arxiv.org/abs/2011.05864" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2011.05864<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> Code: <a href="https://github.com/bohanli/BERT-flow" target="_blank" rel="noopener noreferrer">https://github.com/bohanli/BERT-flow<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><h2 id="background" tabindex="-1"><a class="header-anchor" href="#background" aria-hidden="true">#</a> Background</h2><p>Recently, pre-trained language models and its variants like BERT have been widely used as representations of natural language.</p><p><img src="/blog/assets/image1.25f11b5f.png" alt="image1"></p><p>Photo credit to https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/</p><p>Despite their great success on many NLP tasks through fine-tuning, the sentence embeddings from BERT without finetuning are significantly inferior in terms of semantic textual similarity (Reimers and Gurevych, 2019) – for example, they even underperform the GloVe embeddings which are not contextualized and trained with a much simpler model. Such issues hinder applying BERT sentence embeddings directly to many real-world scenarios where collecting labeled data is highlycosting or even intractable.</p><p><img src="/blog/assets/image2.e005214f.png" alt="image2"></p><h2 id="major-questions" tabindex="-1"><a class="header-anchor" href="#major-questions" aria-hidden="true">#</a> Major Questions</h2><p>In this paper, we aim to answer two major questions:</p><ul><li><p>(1) why do the BERT-induced sentence embeddings perform poorly to retrieve semantically similar sentences? Do they carry too little semantic information, or just because the semantic meanings in these embeddings are not exploited properly?</p></li><li><p>(2) If the BERT embeddings capture enough semantic information that is hard to be directly utilized, how can we make it easier without external supervision?</p></li></ul><h2 id="our-findings" tabindex="-1"><a class="header-anchor" href="#our-findings" aria-hidden="true">#</a> Our Findings</h2><p>We argue that the semantic information in the BERT embeddings is not fully exploited. We first reveal the theoretical connection between the masked language model pre-training objective and the semantic similarity task theoretically, and then analyze the BERT sentence embeddings empirically. We find that BERT always induces a non-smooth anisotropic semantic space of sentences, which harms its performance of semantic similarity.</p><h3 id="the-anisotropic-embedding-space-of-bert" tabindex="-1"><a class="header-anchor" href="#the-anisotropic-embedding-space-of-bert" aria-hidden="true">#</a> The Anisotropic Embedding Space of BERT</h3><p>Gao et al. (2019) and Wang et al. (2020) have pointed out that, for language modeling, the maximum likelihood training with Equation 1 usually produces an anisotropic word embedding space. “Anisotropic” means word embeddings occupy a narrow cone in the vector space. This phenomenon is also observed in the pretrained Transformers like BERT, GPT-2, etc (Ethayarajh, 2019).</p><p><img src="/blog/assets/image3.73f91ab8.png" alt="image3"></p><div class="custom-container tip"><p class="custom-container-title">Tips</p><p>The BERT word embedding space. The 2D-scatterplot is achieved via SVD-based dimension reduction. The embeddings are colored according to their associated word frequency.</p></div><h3 id="word-frequency-biases-the-embedding-space" tabindex="-1"><a class="header-anchor" href="#word-frequency-biases-the-embedding-space" aria-hidden="true">#</a> Word Frequency Biases the Embedding Space</h3><p>However, as discussed by Gao et al. (2019), anisotropy is highly relevant to the imbalance of word frequency. We observe that high-frequency words are all close to the origin, while low-frequency words are far away from the origin.</p><p>This phenomenon can be explained through the softmax formulation of (masked) language models. Note that there is a word-frequency term in the decomposition of the dot product between context and word embeddings. Nevertheless, the PMI term is still highly associated with semantic similarity.</p><div style="text-align:center;"><img src="/blog/assets/image4.413904dd.png" width="400"><img src="/blog/assets/image5.57481ab1.png" width="400"></div><div class="custom-container warning"><p class="custom-container-title">Remark</p><p>We expect the embedding induced similarity to be consistent to semantic similarity. If embeddings are distributed in different regions according to frequency statistics, the induced similarity is not useful any more.</p></div><p><img src="/blog/assets/image6.6e66cf05.png" alt="image6"></p><h3 id="low-frequency-words-disperse-sparsely" tabindex="-1"><a class="header-anchor" href="#low-frequency-words-disperse-sparsely" aria-hidden="true">#</a> Low-Frequency Words Disperse Sparsely</h3><p>We also observe that, in the learned anisotropic embedding space, high-frequency words concentrates densely to their k-nearest neighbors and low-frequency words disperse sparsely.</p><p><img src="/blog/assets/image7.597c467e.png" alt="image7"></p><div class="custom-container warning"><p class="custom-container-title">Remark</p><p>Due to the sparsity, many “holes” could be formed around the low-frequency words in the embedding space, where the semantic meaning can be poorly defined.</p></div><h2 id="proposed-method-bert-flow" tabindex="-1"><a class="header-anchor" href="#proposed-method-bert-flow" aria-hidden="true">#</a> Proposed Method: BERT-flow</h2><p>To address these issues, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an unsupervised objective.</p><p>A standard Gaussian latent space may have favorable properties which can help with our problem.</p><ul><li>First, standard Gaussian satisfies isotropy. By fitting a mapping to an isotropic distribution, the singular spectrum of the embedding space can be flattened. In this way, the word frequency-related singular directions, which are the dominating ones, can be suppressed.</li><li>Second, the probabilistic density of Gaussian is well defined over the entire real space. This means there are no “hole” areas, which are poorly defined in terms of probability. The helpfulness of Gaussian prior for mitigating the “hole” problem has been widely observed in existing literature of deep latent variable models (e.g., variational auto-encoders).</li></ul><p><img src="/blog/assets/image8.85931773.png" alt="image8"></p><h2 id="experiments" tabindex="-1"><a class="header-anchor" href="#experiments" aria-hidden="true">#</a> Experiments</h2><p>Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks.</p><h3 id="results-w-o-nli-supervision" tabindex="-1"><a class="header-anchor" href="#results-w-o-nli-supervision" aria-hidden="true">#</a> Results w/o NLI Supervision</h3><p>We perform extensive experiments on 7 standard semantic textual similarity benchmarks without using any downstream supervision. Our empirical results demonstrate that the flow transformation is able to consistently improve BERT by up to 12.70 points with an average of 8.16 points in terms of Spearman correlation between cosine embedding similarity and human annotated similarity.</p><p><img src="/blog/assets/image9.730eb1eb.png" alt="image9"></p><h3 id="results-w-nli-supervision" tabindex="-1"><a class="header-anchor" href="#results-w-nli-supervision" aria-hidden="true">#</a> Results w/ NLI Supervision</h3><p>When combined with external supervision from NLI tasks, our method outperforms the <strong>sentence-BERT</strong> embeddings (Reimers and Gurevych, 2019), leading to new state-of-theart performance.</p><p><img src="/blog/assets/image10.4085b287.png" alt="image10"></p><h2 id="conclusion" tabindex="-1"><a class="header-anchor" href="#conclusion" aria-hidden="true">#</a> Conclusion</h2><p>We investigate the deficiency of the BERT sentence embeddings on semantic textual similarity. We propose a flow-based calibration which can effectively improve the performance. BERT-flow obtains significant performance gains over the SoTA sentence embeddings on a variety of semantic textual similarity tasks.</p><h2 id="reference" tabindex="-1"><a class="header-anchor" href="#reference" aria-hidden="true">#</a> Reference</h2><ul><li>Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang, Yiming Yang, Lei Li. On the Sentence Embeddings from Pre-trained Language Models. EMNLP 2020.</li><li>Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using siamese BERT networks. In Proceedings of EMNLP-IJCNLP.</li><li>Jun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, and TieYan Liu. 2019. Representation degeneration problem in training natural language generation models. In Proceedings of ICLR.</li><li>Lingxiao Wang, Jing Huang, Kevin Huang, Ziniu Hu, Guangtao Wang, and Quanquan Gu. 2020. Improving neural language generation with spectrum control. In Proceedings of ICLR.</li><li>Kawin Ethayarajh. 2019. How contextual are contextualized word representations? comparing the geometry of bert, elmo, and gpt-2 embeddings. In Proceedings of EMNLP-IJCNLP.</li></ul></div><!----><footer class="page-meta"><div class="meta-item edit-link"><a href="https://github.com/lileicc/blog/edit/main/nlp/bert-flow/README.md" rel="noopener noreferrer" target="_blank" aria-label="Edit this page" class="nav-link label"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="edit icon"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->Edit this page<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!----></a></div><div class="meta-item update-time"><span class="label">Last update: </span><span class="info">9/13/2022, 3:45:15 AM</span></div><div class="meta-item contributors"><span class="label">Contributors: </span><!--[--><!--[--><span class="contributor" title="email: lileicc@gmail.com">Lei Li</span><!--]--><!--]--></div></footer><!----><div class="giscus-wrapper input-top" style="display:block;"><div style="text-align:center">Loading...</div></div><!----><!--]--></main><!--]--><footer class="footer-wrapper"><div class="footer">Li Lab</div><div class="copyright">Copyright © 2023 Bohan Li</div></footer><!--]--></div><!--]--><!----><!--]--></div>
    <script type="module" src="/blog/assets/app.4a645708.js" defer></script>
  </body>
</html>
