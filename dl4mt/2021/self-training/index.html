<!DOCTYPE html>
<html lang="en-US" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-beta.51" />
    <meta name="theme" content="VuePress Theme Hope" />
    <meta property="og:url" content="https://lileicc.github.io/blog/dl4mt/2021/self-training/"><meta property="og:site_name" content="MLNLP Blog"><meta property="og:title" content="Revisiting Self-training for Neural Sequence Generation"><meta property="og:type" content="article"><meta property="og:image" content="https://lileicc.github.io/blog/"><meta property="og:updated_time" content="2022-09-13T03:45:15.000Z"><meta property="og:locale" content="en-US"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image:alt" content="Revisiting Self-training for Neural Sequence Generation"><meta property="article:author" content="Zekun Li"><meta property="article:tag" content="Self-training"><meta property="article:published_time" content="2021-12-05T00:00:00.000Z"><meta property="article:modified_time" content="2022-09-13T03:45:15.000Z"><title>Revisiting Self-training for Neural Sequence Generation | MLNLP Blog</title><meta name="description" content="A Blog for Machine Learning, Natural Language Processing, and Data Mining">
    <style>
      :root {
        --bg-color: #fff;
      }

      html[data-theme="dark"] {
        --bg-color: #1d2025;
      }

      html,
      body {
        background-color: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.querySelector("html").setAttribute("data-theme", "dark");
      }
    </script>
    <link rel="stylesheet" href="/blog/assets/style.758fbe27.css">
    <link rel="modulepreload" href="/blog/assets/app.503dafa8.js"><link rel="modulepreload" href="/blog/assets/index.html.ad236b80.js"><link rel="modulepreload" href="/blog/assets/_plugin-vue_export-helper.cdc0426e.js"><link rel="modulepreload" href="/blog/assets/index.html.9a21b56f.js"><link rel="prefetch" href="/blog/assets/index.html.48d72793.js"><link rel="prefetch" href="/blog/assets/index.html.a49e14cc.js"><link rel="prefetch" href="/blog/assets/index.html.84f09450.js"><link rel="prefetch" href="/blog/assets/index.html.8cadf27e.js"><link rel="prefetch" href="/blog/assets/index.html.948f6ba7.js"><link rel="prefetch" href="/blog/assets/index.html.4e8ce27b.js"><link rel="prefetch" href="/blog/assets/index.html.cc204d40.js"><link rel="prefetch" href="/blog/assets/index.html.9d3d23de.js"><link rel="prefetch" href="/blog/assets/index.html.e207b34f.js"><link rel="prefetch" href="/blog/assets/index.html.6fe109c4.js"><link rel="prefetch" href="/blog/assets/index.html.f2001ec9.js"><link rel="prefetch" href="/blog/assets/index.html.f13e23f3.js"><link rel="prefetch" href="/blog/assets/index.html.36a209ea.js"><link rel="prefetch" href="/blog/assets/index.html.e51071bb.js"><link rel="prefetch" href="/blog/assets/index.html.3b54c059.js"><link rel="prefetch" href="/blog/assets/index.html.9bc33622.js"><link rel="prefetch" href="/blog/assets/index.html.1b2f4c5b.js"><link rel="prefetch" href="/blog/assets/index.html.2693264c.js"><link rel="prefetch" href="/blog/assets/index.html.371141f3.js"><link rel="prefetch" href="/blog/assets/index.html.e3b8fd71.js"><link rel="prefetch" href="/blog/assets/index.html.3cc17b99.js"><link rel="prefetch" href="/blog/assets/index.html.d6d3e578.js"><link rel="prefetch" href="/blog/assets/index.html.1f3a298a.js"><link rel="prefetch" href="/blog/assets/404.html.144ea56c.js"><link rel="prefetch" href="/blog/assets/index.html.3a32b4c3.js"><link rel="prefetch" href="/blog/assets/index.html.3eaba85c.js"><link rel="prefetch" href="/blog/assets/index.html.9c8e527a.js"><link rel="prefetch" href="/blog/assets/index.html.949c6932.js"><link rel="prefetch" href="/blog/assets/index.html.9369be8b.js"><link rel="prefetch" href="/blog/assets/index.html.5ecb22f0.js"><link rel="prefetch" href="/blog/assets/index.html.0a31196e.js"><link rel="prefetch" href="/blog/assets/index.html.1b68751c.js"><link rel="prefetch" href="/blog/assets/index.html.85fce0bc.js"><link rel="prefetch" href="/blog/assets/index.html.7d865fc8.js"><link rel="prefetch" href="/blog/assets/index.html.73acbfb4.js"><link rel="prefetch" href="/blog/assets/index.html.7e45f7bf.js"><link rel="prefetch" href="/blog/assets/index.html.620dcedf.js"><link rel="prefetch" href="/blog/assets/index.html.83d0711e.js"><link rel="prefetch" href="/blog/assets/index.html.b4a0aa08.js"><link rel="prefetch" href="/blog/assets/index.html.7adb01cb.js"><link rel="prefetch" href="/blog/assets/index.html.f005e561.js"><link rel="prefetch" href="/blog/assets/index.html.7fb0dc28.js"><link rel="prefetch" href="/blog/assets/index.html.71f8befa.js"><link rel="prefetch" href="/blog/assets/index.html.92c68d7e.js"><link rel="prefetch" href="/blog/assets/index.html.055d7455.js"><link rel="prefetch" href="/blog/assets/index.html.df6f3f04.js"><link rel="prefetch" href="/blog/assets/index.html.00dbedea.js"><link rel="prefetch" href="/blog/assets/index.html.603bf394.js"><link rel="prefetch" href="/blog/assets/index.html.cd4b9e10.js"><link rel="prefetch" href="/blog/assets/index.html.be0f419f.js"><link rel="prefetch" href="/blog/assets/index.html.bc51d0ae.js"><link rel="prefetch" href="/blog/assets/index.html.b79794f0.js"><link rel="prefetch" href="/blog/assets/index.html.eb8e4cee.js"><link rel="prefetch" href="/blog/assets/index.html.5c842c2c.js"><link rel="prefetch" href="/blog/assets/index.html.a95edb3c.js"><link rel="prefetch" href="/blog/assets/index.html.13db0b56.js"><link rel="prefetch" href="/blog/assets/index.html.bad01ddf.js"><link rel="prefetch" href="/blog/assets/index.html.ed77c4d1.js"><link rel="prefetch" href="/blog/assets/index.html.a3514a8d.js"><link rel="prefetch" href="/blog/assets/index.html.65bca0e5.js"><link rel="prefetch" href="/blog/assets/index.html.5bb552c5.js"><link rel="prefetch" href="/blog/assets/index.html.6eb9e54e.js"><link rel="prefetch" href="/blog/assets/index.html.62d61c88.js"><link rel="prefetch" href="/blog/assets/index.html.1362a299.js"><link rel="prefetch" href="/blog/assets/index.html.68f9bc07.js"><link rel="prefetch" href="/blog/assets/index.html.0272cd41.js"><link rel="prefetch" href="/blog/assets/index.html.3b47eb32.js"><link rel="prefetch" href="/blog/assets/index.html.d8dad1e7.js"><link rel="prefetch" href="/blog/assets/index.html.f01de255.js"><link rel="prefetch" href="/blog/assets/index.html.415c7585.js"><link rel="prefetch" href="/blog/assets/index.html.c876a430.js"><link rel="prefetch" href="/blog/assets/index.html.1b3b1bb2.js"><link rel="prefetch" href="/blog/assets/index.html.d1ba8a2a.js"><link rel="prefetch" href="/blog/assets/index.html.ba6b6066.js"><link rel="prefetch" href="/blog/assets/index.html.59861a75.js"><link rel="prefetch" href="/blog/assets/index.html.1cd9b78a.js"><link rel="prefetch" href="/blog/assets/index.html.fc86ee0a.js"><link rel="prefetch" href="/blog/assets/index.html.5c2c55d2.js"><link rel="prefetch" href="/blog/assets/index.html.5e7d0d3d.js"><link rel="prefetch" href="/blog/assets/index.html.ff55cc35.js"><link rel="prefetch" href="/blog/assets/index.html.90cde4cc.js"><link rel="prefetch" href="/blog/assets/index.html.087c62bd.js"><link rel="prefetch" href="/blog/assets/index.html.155961dc.js"><link rel="prefetch" href="/blog/assets/index.html.7f990628.js"><link rel="prefetch" href="/blog/assets/index.html.36645606.js"><link rel="prefetch" href="/blog/assets/index.html.d5e82883.js"><link rel="prefetch" href="/blog/assets/index.html.f3488434.js"><link rel="prefetch" href="/blog/assets/index.html.555143a6.js"><link rel="prefetch" href="/blog/assets/index.html.fe8e618e.js"><link rel="prefetch" href="/blog/assets/index.html.d5e9784e.js"><link rel="prefetch" href="/blog/assets/index.html.e417b753.js"><link rel="prefetch" href="/blog/assets/index.html.3f123c28.js"><link rel="prefetch" href="/blog/assets/index.html.2a083ff8.js"><link rel="prefetch" href="/blog/assets/index.html.da71d353.js"><link rel="prefetch" href="/blog/assets/index.html.8bc7942d.js"><link rel="prefetch" href="/blog/assets/index.html.6a3bdaae.js"><link rel="prefetch" href="/blog/assets/index.html.8b67503f.js"><link rel="prefetch" href="/blog/assets/index.html.bb190627.js"><link rel="prefetch" href="/blog/assets/404.html.78e4d99b.js"><link rel="prefetch" href="/blog/assets/index.html.6279babe.js"><link rel="prefetch" href="/blog/assets/index.html.b446c064.js"><link rel="prefetch" href="/blog/assets/index.html.f9393cf8.js"><link rel="prefetch" href="/blog/assets/index.html.d276116d.js"><link rel="prefetch" href="/blog/assets/index.html.3c5e5245.js"><link rel="prefetch" href="/blog/assets/index.html.e0a5add6.js"><link rel="prefetch" href="/blog/assets/index.html.751aefd7.js"><link rel="prefetch" href="/blog/assets/index.html.46853238.js"><link rel="prefetch" href="/blog/assets/index.html.1423193a.js"><link rel="prefetch" href="/blog/assets/index.html.0c030957.js"><link rel="prefetch" href="/blog/assets/index.html.d663e501.js"><link rel="prefetch" href="/blog/assets/index.html.38ff1351.js"><link rel="prefetch" href="/blog/assets/index.html.66c26498.js"><link rel="prefetch" href="/blog/assets/index.html.38c9451d.js"><link rel="prefetch" href="/blog/assets/index.html.6795f417.js"><link rel="prefetch" href="/blog/assets/index.html.0b2f032c.js"><link rel="prefetch" href="/blog/assets/index.html.a3eee5b6.js"><link rel="prefetch" href="/blog/assets/index.html.78ac4d64.js"><link rel="prefetch" href="/blog/assets/index.html.fb4da446.js"><link rel="prefetch" href="/blog/assets/index.html.de75c74c.js"><link rel="prefetch" href="/blog/assets/index.html.f5635468.js"><link rel="prefetch" href="/blog/assets/index.html.a6972c60.js"><link rel="prefetch" href="/blog/assets/index.html.82348866.js"><link rel="prefetch" href="/blog/assets/index.html.3ea270e5.js"><link rel="prefetch" href="/blog/assets/index.html.4eba6eae.js"><link rel="prefetch" href="/blog/assets/index.html.fdb8b59d.js"><link rel="prefetch" href="/blog/assets/index.html.000f742d.js"><link rel="prefetch" href="/blog/assets/index.html.3005042c.js"><link rel="prefetch" href="/blog/assets/index.html.75eaa272.js"><link rel="prefetch" href="/blog/assets/index.html.884e31d3.js"><link rel="prefetch" href="/blog/assets/index.html.7e7aa3ad.js"><link rel="prefetch" href="/blog/assets/index.html.cab1ab73.js"><link rel="prefetch" href="/blog/assets/index.html.50db4479.js"><link rel="prefetch" href="/blog/assets/index.html.ca6588e0.js"><link rel="prefetch" href="/blog/assets/index.html.e96e6af0.js"><link rel="prefetch" href="/blog/assets/index.html.44c19c07.js"><link rel="prefetch" href="/blog/assets/index.html.5e705310.js"><link rel="prefetch" href="/blog/assets/index.html.a20fcb1f.js"><link rel="prefetch" href="/blog/assets/index.html.c3bf8232.js"><link rel="prefetch" href="/blog/assets/index.html.93033562.js"><link rel="prefetch" href="/blog/assets/index.html.7bf7cb3c.js"><link rel="prefetch" href="/blog/assets/index.html.13c3f4f0.js"><link rel="prefetch" href="/blog/assets/index.html.469fac95.js"><link rel="prefetch" href="/blog/assets/index.html.408ac946.js"><link rel="prefetch" href="/blog/assets/index.html.ce8363a9.js"><link rel="prefetch" href="/blog/assets/index.html.11e153f9.js"><link rel="prefetch" href="/blog/assets/index.html.6f2c8562.js"><link rel="prefetch" href="/blog/assets/index.html.d7705070.js"><link rel="prefetch" href="/blog/assets/index.html.0510738e.js"><link rel="prefetch" href="/blog/assets/index.html.52b40114.js"><link rel="prefetch" href="/blog/assets/index.html.acbcdfb7.js"><link rel="prefetch" href="/blog/assets/giscus.15440425.js"><link rel="prefetch" href="/blog/assets/highlight.esm.d982e650.js"><link rel="prefetch" href="/blog/assets/markdown.esm.832a189d.js"><link rel="prefetch" href="/blog/assets/math.esm.a3f84b6f.js"><link rel="prefetch" href="/blog/assets/notes.esm.3c361cb7.js"><link rel="prefetch" href="/blog/assets/reveal.esm.b96f05d8.js"><link rel="prefetch" href="/blog/assets/search.esm.80da4a02.js"><link rel="prefetch" href="/blog/assets/zoom.esm.8514a202.js"><link rel="prefetch" href="/blog/assets/photoswipe.esm.382b1873.js">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="skip-link sr-only">Skip to content</a><!--]--><div class="theme-container no-sidebar has-toc"><!--[--><!--[--><header class="navbar"><div class="navbar-left"><button class="toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!----><a href="/blog/" class="brand"><img class="logo" src="/blog/logo.svg" alt="MLNLP Blog"><!----><span class="site-name hide-in-pad">MLNLP Blog</span></a><!----></div><div class="navbar-center"><!----><nav class="nav-links"><div class="nav-item hide-in-mobile"><a href="/blog/" class="nav-link" aria-label="Blog Home"><span class="icon iconfont icon-home"></span>Blog Home<!----></a></div><div class="nav-item hide-in-mobile"><a href="/blog/category/" class="nav-link" aria-label="Category"><span class="icon iconfont icon-categoryselected"></span>Category<!----></a></div><div class="nav-item hide-in-mobile"><a href="/blog/tag/" class="nav-link" aria-label="Tags"><span class="icon iconfont icon-tag"></span>Tags<!----></a></div><div class="nav-item hide-in-mobile"><a href="/blog/timeline/" class="nav-link" aria-label="Timeline"><span class="icon iconfont icon-time"></span>Timeline<!----></a></div></nav><!----></div><div class="navbar-right"><!----><!----><div class="nav-item"><a class="repo-link" href="https://github.com/lileicc/blog" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="nav-item hide-in-mobile"><button id="appearance-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="light icon" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><form class="search-box" role="search"><input type="search" autocomplete="off" spellcheck="false" value><!----></form><!----><button class="toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span class="button-container"><span class="button-top"></span><span class="button-middle"></span><span class="button-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow left"></span></div><aside class="sidebar"><!--[--><!----><!--]--><ul class="sidebar-links"></ul><!--[--><!----><!--]--></aside><!--[--><main class="page" id="main-content"><!--[--><!----><nav class="breadcrumb disable"></nav><div class="page-title"><h1><!---->Revisiting Self-training for Neural Sequence Generation</h1><div class="page-info"><span class="author-info" aria-label="Author🖊" data-balloon-pos="down" localizeddate="December 5, 2021" isoriginal="false" pageview="false"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><span class="author-item">Zekun Li</span></span><span property="author" content="Zekun Li"></span></span><!----><span class="date-info" aria-label="Writing Date📅" data-balloon-pos="down" isoriginal="false" pageview="false"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span>December 5, 2021</span><meta property="datePublished" content="2021-12-05T00:00:00.000Z"></span><span class="category-info" aria-label="Category🌈" data-balloon-pos="down" localizeddate="December 5, 2021" isoriginal="false" pageview="false"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="category icon"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><ul class="categories-wrapper"><li class="category category4 clickable" role="navigation">MT</li><li class="category category1 clickable" role="navigation">DL4MT</li><meta property="articleSection" content="MT,DL4MT"></ul></span><span aria-label="Tag🏷" data-balloon-pos="down" localizeddate="December 5, 2021" isoriginal="false" pageview="false"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="tag icon"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><ul class="tags-wrapper"><li class="tag tag0 clickable" role="navigation">Self-training</li></ul><meta property="keywords" content="Self-training"></span><span class="reading-time-info" aria-label="Reading Time⌛" data-balloon-pos="down" localizeddate="December 5, 2021" isoriginal="false" pageview="false"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>About 5 min</span><meta property="timeRequired" content="PT5M"></span></div><hr></div><div class="toc-place-holder"><aside id="toc"><div class="toc-header">On This Page</div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a aria-current="page" href="/blog/dl4mt/2021/self-training/#_1-introduction" class="router-link-active router-link-exact-active toc-link level2">1. Introduction</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/blog/dl4mt/2021/self-training/#_2-case-study-on-machine-translation" class="router-link-active router-link-exact-active toc-link level2">2. Case Study on Machine Translation</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/blog/dl4mt/2021/self-training/#_3-the-secret-behind-self-training" class="router-link-active router-link-exact-active toc-link level2">3. The Secret Behind Self-training</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/blog/dl4mt/2021/self-training/#_4-the-proposed-method-noisy-self-training" class="router-link-active router-link-exact-active toc-link level2">4. The Proposed Method: Noisy Self-training</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/blog/dl4mt/2021/self-training/#_5-experiments" class="router-link-active router-link-exact-active toc-link level2">5. Experiments</a></li><ul class="toc-list"><!--[--><li class="toc-item"><a aria-current="page" href="/blog/dl4mt/2021/self-training/#machine-translation" class="router-link-active router-link-exact-active toc-link level3">Machine Translation</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/blog/dl4mt/2021/self-training/#analysis" class="router-link-active router-link-exact-active toc-link level3">Analysis</a></li><!----><!--]--></ul><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/blog/dl4mt/2021/self-training/#summary" class="router-link-active router-link-exact-active toc-link level2">Summary</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/blog/dl4mt/2021/self-training/#references" class="router-link-active router-link-exact-active toc-link level2">References</a></li><!----><!--]--></ul></div></aside></div><!----><div class="theme-hope-content"><p>Self-training is a very prevalent semi-supervised method. Its key idea is to augment the original labeled dataset with unlabeled data paired with the model&#39;s prediction (i.e. the <em>pseudo-parallel</em> data). Self-training has been widely used in classification tasks. However, will it work on sequence generation tasks (e.g. machine translation)? If so, how does it work? This blog introduces a work [1] which investigates these questions and gives the answers.</p><!-- more --><p>Reading Time: About 10 minutes.</p><p>Paper：https://arxiv.org/abs/1909.13788</p><p>Github: https://github.com/jxhe/self-training-text-generation</p><h2 id="_1-introduction" tabindex="-1"><a class="header-anchor" href="#_1-introduction" aria-hidden="true">#</a> 1. Introduction</h2><p><img src="/blog/assets/self-training.cf6e07e1.jpg" alt="image1"> Deep neural networks often require large amounts of labeled data to achieve good performance. However, it is very costly to acquire labels. So what if there is not enough labeled data? Researchers try to fully utilize the unlabeled data to improve the model performance. Self-training is a simple but effective method. As can be seen in the figure above, in self-training, a base model trained with labeled data acts as a “teacher” to label the unannotated data, which is then used to augment the original small training set. Then, a “student” model is trained with this new training set to yield the final model. Self-training is originally designed for classification problems, and it is believed that this method may be effective only when a good fraction of the predictions on unlabeled samples are correct, otherwise errors will be accumulated.</p><p>However, self-training has not been studied extensively in neural sequence generation tasks like machine translation, where the target output is natural language. So the question arises: can self-training still be useful in this case? Here we introduce a work [1] which investigate the problem and answer the two questions:</p><ol><li>How does self-training perform in sequence generation tasks like machine translation?</li><li>If self-training helps improving the baseline, what contributes to its success?</li></ol><h2 id="_2-case-study-on-machine-translation" tabindex="-1"><a class="header-anchor" href="#_2-case-study-on-machine-translation" aria-hidden="true">#</a> 2. Case Study on Machine Translation</h2><p>The authors first analyze the machine translation task, and then perform ablation analysis to understand the contributing factors of the performance gains.</p><p>They work with the standard WMT 2014 English-German dataset. As a preliminary experiment, they randomly sample 100K sentences from the training set (WMT100K) and use the remaining English sentences as the unlabeled monolingual data. They train with the Base Transformer architecture and use beam search decoding (beam size 5).</p><p><img src="/blog/assets/bar.9ec0f4a9.png" alt="image2"> Green bars in the above figure shows the result of applying self-training for three iterations, which includes:</p><ol><li>Pseudo-training (PT): the first step of self-training where we train a new model (from scratch) using only the pseudo parallel data generated by the current model</li><li>Fine-tuning (FT): the fine-tuned system using real parallel data based on the pretrained model from the PT step.</li></ol><p>It is surprising that the pseudo-training step at the first iteration is able to improve BLEU even if the model is only trained on its own predictions, and fine-tuning further boosts the performance. An explanation is that the added pseudo-parallel data might implicitly change the training trajectory towards a (somehow) better local optimum, given that we train a new model from scratch at each iteration.</p><table><thead><tr><th style="text-align:left;">Methods</th><th style="text-align:center;">PT</th><th style="text-align:center;">FT</th></tr></thead><tbody><tr><td style="text-align:left;">baseline</td><td style="text-align:center;">-</td><td style="text-align:center;">15.6</td></tr><tr><td style="text-align:left;">baseline (w/o dropout)</td><td style="text-align:center;">-</td><td style="text-align:center;">5.2</td></tr><tr><td style="text-align:left;">ST (beam search, w/ dropout)</td><td style="text-align:center;">16.5</td><td style="text-align:center;">17.5</td></tr><tr><td style="text-align:left;">ST (sampling, w/ dropout)</td><td style="text-align:center;">16.1</td><td style="text-align:center;">17.0</td></tr><tr><td style="text-align:left;">ST (beam search, w/o dropout)</td><td style="text-align:center;">15.8</td><td style="text-align:center;">16.3</td></tr><tr><td style="text-align:left;">ST (sampling, w/o dropout)</td><td style="text-align:center;">15.5</td><td style="text-align:center;">16.0</td></tr><tr><td style="text-align:left;">Noisy ST (beam search, w/o dropout)</td><td style="text-align:center;">15.8</td><td style="text-align:center;">17.9</td></tr><tr><td style="text-align:left;">Noisy ST (beam search, w/ dropout)</td><td style="text-align:center;"><strong>16.6</strong></td><td style="text-align:center;"><strong>19.3</strong></td></tr></tbody></table><h2 id="_3-the-secret-behind-self-training" tabindex="-1"><a class="header-anchor" href="#_3-the-secret-behind-self-training" aria-hidden="true">#</a> 3. The Secret Behind Self-training</h2><p>To decode the secret of self-training and understand where the gain comes from, they formulate two hypotheses:</p><ol><li><p><strong>Decoding Strategy</strong>: According to this hypothesis, the gains come from the use of beam search for decoding unlabeled data. The above table shows the performance using different decoding strategies. As can be seen, the performance drops by 0.5 BLEU when the decoding strategy is changed to sampling, which implies that beam search does contribute a bit to the performance gains. This phenomenon makes sense intuitively since beam search tends to generate higher-quality pseudo targets than sampling. However, the decoding strategy hypothesis does not fully explain it, as there is still a gain of 1.4 BLEU points over the baseline from sampling decoding with dropout.</p></li><li><p><strong>Dropout</strong>: The results in the above table indicate that without dropout the performance of beam search decoding drops by 1.2 BLEU, just 0.7 BLEU higher than the baseline. Moreover, the pseudo-training performance of sampling without dropout is almost the same as the baseline.</p></li></ol><p>In summary, beam-search decoding contributes only partially to the performance gains, while the implicit perturbation i.e., dropout accounts for most of it. The authors also conduct experiment on a toy dataset to show that noise is beneficial for self-training because it enforces local smoothness for this task, that is, semantically similar inputs are mapped to the same or similar targets.</p><h2 id="_4-the-proposed-method-noisy-self-training" tabindex="-1"><a class="header-anchor" href="#_4-the-proposed-method-noisy-self-training" aria-hidden="true">#</a> 4. The Proposed Method: Noisy Self-training</h2><p>To further improve performance, the authors considers a simple model-agnostic perturbation process - perturbing the input, which is referred to as <em>noisy self-training</em>. Note that they apply both input perturbation and dropout in the pseudo-training step for noisy ST. They first apply noisy ST to the WMT100K translation task. Two different perturbation function are tested:</p><ol><li>Synthetic noise: the input tokens are randomly dropped, masked, and shuffled.</li><li>Paraphrase: they translate the source English sentences to German and translate it back to obtain a paraphrase as the perturbation.</li></ol><p>Figure 2 shows the results over three iterations. Noisy ST greatly outperforms the supervised baseline and normal ST, while synthetic noise does not exhibit much difference from paraphrase. Since synthetic noise is much simpler and more general, it is defaulted in Noisy ST. Table 1 also reports an ablation study of Noisy ST when removing dropout at the pseudo-training step. Noisy ST without dropout improves the baseline by 2.3 BLEU points and is comparable to normal ST with dropout. When combined together, noisy ST with dropout produces another 1.4 BLEU improvement, indicating that the two perturbations are complementary.</p><h2 id="_5-experiments" tabindex="-1"><a class="header-anchor" href="#_5-experiments" aria-hidden="true">#</a> 5. Experiments</h2><h3 id="machine-translation" tabindex="-1"><a class="header-anchor" href="#machine-translation" aria-hidden="true">#</a> Machine Translation</h3><p>The author test the proposed noisy ST on a high-resource MT benchmark: WMT14 English-German and a low-resource one: FloRes English-Nepali.</p><table><thead><tr><th style="text-align:left;">Methods</th><th style="text-align:center;">WMT14 100K</th><th style="text-align:center;">WMT14 3.9M</th><th style="text-align:center;">FloRes En-Origin</th><th style="text-align:center;">FloRes Ne-Origin</th><th style="text-align:center;">FloRes Overall</th></tr></thead><tbody><tr><td style="text-align:left;">baseline</td><td style="text-align:center;">15.6</td><td style="text-align:center;">28.3</td><td style="text-align:center;">6.7</td><td style="text-align:center;">2.3</td><td style="text-align:center;">4.8</td></tr><tr><td style="text-align:left;">BT</td><td style="text-align:center;">20.5</td><td style="text-align:center;">-</td><td style="text-align:center;">8.2</td><td style="text-align:center;"><strong>4.5</strong></td><td style="text-align:center;"><strong>6.5</strong></td></tr><tr><td style="text-align:left;">Noisy ST</td><td style="text-align:center;"><strong>21.4</strong></td><td style="text-align:center;"><strong>29.3</strong></td><td style="text-align:center;"><strong>8.9</strong></td><td style="text-align:center;">3.5</td><td style="text-align:center;"><strong>6.5</strong></td></tr></tbody></table><p>The overall results are shown in the above table. For almost all cases in both datasets, the noisy ST outperforms the baselines by a large margin, and noisy ST still improves the baseline even when this is very weak.</p><h4 id="comparison-with-back-translation" tabindex="-1"><a class="header-anchor" href="#comparison-with-back-translation" aria-hidden="true">#</a> Comparison with Back Translation</h4><p>It can be seen that noisy ST is able to beat BT on WMT100K and on the en-origin test set of FloRes. In contrast, BT is more effective on the ne-origin test set according to BLEU, which is not surprising as the ne-origin test is likely to benefit more from Nepali than English monolingual data.</p><p><img src="/blog/assets/analysis.fafbefaa.png" alt="image3"></p><h3 id="analysis" tabindex="-1"><a class="header-anchor" href="#analysis" aria-hidden="true">#</a> Analysis</h3><p>The authors analyze the effect of the following three factors on noisy self-training on the WMT14 dataset:</p><ol><li>Parallel dat size</li><li>Monolingual dat size</li><li>Noise level The result is shown in the above figure. In (a) we see that the performance gain is larger for intermediate value of the size of the parallel dataset, as expected. (b) illustrates that the performance keeps improving as the monolingual data size increases, albeit with diminishing returns. (c) demonstrates that performance is quite sensitive to noise level, and that intermediate values work best. It is still unclear how to select the noise level a priori, besides the usual hyper-parameter search to maximize BLEU on the validation set.</li></ol><h2 id="summary" tabindex="-1"><a class="header-anchor" href="#summary" aria-hidden="true">#</a> Summary</h2><p>This work revisit self-training for neural sequence generation, especially machine translation task. It is shown that self-training can be an effective method to improve generalization, particularly when labeled data is scarce. Through comprehensive experiments, they prove that noise injected during self-training is critical and thus propose to perturb the input to obtain a variant of self-training, named noisy self-training, which show great power on machine translation and also text summarization tasks.</p><h2 id="references" tabindex="-1"><a class="header-anchor" href="#references" aria-hidden="true">#</a> References</h2><p>[1] He, Junxian, et al. &quot;Revisiting Self-Training for Neural Sequence Generation.&quot; International Conference on Learning Representations. 2019.</p></div><!----><footer class="page-meta"><div class="meta-item edit-link"><a href="https://github.com/lileicc/blog/edit/main/dl4mt/2021/self-training/README.md" rel="noopener noreferrer" target="_blank" aria-label="Edit this page" class="nav-link label"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="edit icon"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->Edit this page<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!----></a></div><div class="meta-item update-time"><span class="label">Last update: </span><span class="info">9/13/2022, 3:45:15 AM</span></div><div class="meta-item contributors"><span class="label">Contributors: </span><!--[--><!--[--><span class="contributor" title="email: lileicc@gmail.com">Lei Li</span><!--]--><!--]--></div></footer><!----><div class="giscus-wrapper input-top" style="display:block;"><div style="text-align:center">Loading...</div></div><!----><!--]--></main><!--]--><footer class="footer-wrapper"><div class="footer">Li Lab</div><div class="copyright">Copyright © 2023 Zekun Li</div></footer><!--]--></div><!--]--><!----><!--]--></div>
    <script type="module" src="/blog/assets/app.503dafa8.js" defer></script>
  </body>
</html>
