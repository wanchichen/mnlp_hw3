import{_ as s}from"./_plugin-vue_export-helper.cdc0426e.js";import{o as i,c as r,b as e,d as n,a as o,e as t,f as l,r as d}from"./app.b901fe8f.js";const h={},c=e("h2",{id:"introduction",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#introduction","aria-hidden":"true"},"#"),t(" Introduction")],-1),u=t("Large Language Models (LLMs) like "),g=e("a",{href:"chat.openai.com"},"ChatGPT",-1),m=t(" have rocked the world over the past year. They are able to perform almost any task you can think of, such as summarization, translation, and story-telling. But how do these LLMs work? And should you use them over existing tools? A "),p={href:"https://arxiv.org/pdf/2304.04675.pdf",target:"_blank",rel:"noopener noreferrer"},f=t("recent study"),y=t(" by AI researchers investigated how LLMs can be used for translation, and evaluate them against the best dedicated translation systems available. Our blog post today will summarize their findings and show how their study can be extended to new languages."),v=t("Paper: "),b={href:"https://arxiv.org/abs/2304.04675",target:"_blank",rel:"noopener noreferrer"},x=t("https://arxiv.org/abs/2304.04675"),L=t("Code: "),w={href:"https://github.com/NJUNLP/MMT-LLM",target:"_blank",rel:"noopener noreferrer"},M=t("https://github.com/NJUNLP/MMT-LLM"),T=l(`<h2 id="in-context-learning" tabindex="-1"><a class="header-anchor" href="#in-context-learning" aria-hidden="true">#</a> In-Context Learning</h2><p>Large-scale machine learning models are trained on an extremely vast amount of data. But when it comes down to actual usage, we typically only need these models to perform certain tasks in certain domains. Thus, it is common to take a model pre-trained on a large general dataset and fine-tune it on a smaller task/domain specific dataset. However, this process is still expensive, as the amount of data necessary to achieve good performance on specific tasks after fine-tuning is still rather large, and the fine-tuning process itself requires access to GPU compute. Can we instead teach a model to perform a task without fine-tuning?</p><p>Recent research has shown that we can do achieve this with, in-context learning (ICL): the concept of showing an LLM a few examples of a task, before asking it to complete the task for a new data. In the context of translation, for example, this can be done by giving the LLM a few example translations, such as:</p><div class="language-text ext-text line-numbers-mode"><pre class="language-text"><code>I love potatoes -&gt; J&#39;aime les pommes de terre
Where did you buy that purse? -&gt; O\xF9 as-tu achet\xE9 ce sac \xE0 main?
You can feed deer at Nara Park -&gt; Vous pouvez nourrir les cerfs au parc de Nara
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Before asking it giving it this task:</p><div class="language-text ext-text line-numbers-mode"><pre class="language-text"><code>Let\u2019s see if its value is mentioned in any other responses. -&gt;
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>As you can probably guess, the examples you give the LLM can significantly affect its ability to perform ICL. The examples need to be represent the different possible ways to properly perform the task. As such, you may need to perform significant amounts of engineering to properly teach the LLM more difficult tasks, such as translation with rare languages or uncommon language pairs. ICL is possible in LLMs due to the nature of their pre-training task: predicting the next word. The examples provided by the user for ICL becomes the context the LLM uses to reatedly predict the next word, evenutally forming an output translation. This makes ICL a property mostly unique to LLMs, as it requires a specific pre-training type, along with a sufficiently large model/dataset.</p><h2 id="experimental-setup" tabindex="-1"><a class="header-anchor" href="#experimental-setup" aria-hidden="true">#</a> Experimental setup</h2><p>One of the main contributions of the paper is that the authors performed extensive analysis of diverse models and diverse languages. We first present the list of models and language they considered in the paper.</p><h3 id="evaluated-models" tabindex="-1"><a class="header-anchor" href="#evaluated-models" aria-hidden="true">#</a> Evaluated Models</h3><p>The study compared eight LLMs (XGLM-7.5B, OPT-175B, Falcon-7B, BLOOMZ-7.1B, LLAMA2-7B, LLAMA2-7B-chat, ChatGPT, and GPT-4) with three dedicated multilingual machine translation (MT)models (M2M-100-12B, NLLB-1.3B, and Google Translate). A comparison is shown in the following table:</p><table><thead><tr><th style="text-align:center;">Model</th><th style="text-align:center;">Type</th><th style="text-align:center;">Parameters</th><th style="text-align:center;">Notes</th></tr></thead><tbody><tr><td style="text-align:center;">XGLM</td><td style="text-align:center;">LLM</td><td style="text-align:center;">7.5B</td><td style="text-align:center;">Multilingual</td></tr><tr><td style="text-align:center;">OPT</td><td style="text-align:center;">LLM</td><td style="text-align:center;">175B</td><td style="text-align:center;"></td></tr><tr><td style="text-align:center;">Falcon</td><td style="text-align:center;">LLM</td><td style="text-align:center;">7B</td><td style="text-align:center;"></td></tr><tr><td style="text-align:center;">BLOOMZ</td><td style="text-align:center;">LLM</td><td style="text-align:center;">7.1B</td><td style="text-align:center;">Multilingual</td></tr><tr><td style="text-align:center;">LLaMA2</td><td style="text-align:center;">LLM</td><td style="text-align:center;">7B</td><td style="text-align:center;"></td></tr><tr><td style="text-align:center;">LLaMA2-chat</td><td style="text-align:center;">LLM</td><td style="text-align:center;">7B</td><td style="text-align:center;">Trained for chatting</td></tr><tr><td style="text-align:center;">ChatGPT</td><td style="text-align:center;">LLM</td><td style="text-align:center;">175B</td><td style="text-align:center;">Trained for chatting</td></tr><tr><td style="text-align:center;">GPT-4</td><td style="text-align:center;">LLM</td><td style="text-align:center;">Unknown</td><td style="text-align:center;">Commercial product, multimodal, trained for chatting</td></tr><tr><td style="text-align:center;">M2M-100</td><td style="text-align:center;">MT</td><td style="text-align:center;">12B</td><td style="text-align:center;">Trained on 100 languages</td></tr><tr><td style="text-align:center;">NLLB</td><td style="text-align:center;">MT</td><td style="text-align:center;">1.3B</td><td style="text-align:center;">Trained on 200 languages</td></tr><tr><td style="text-align:center;">Google Translate</td><td style="text-align:center;">MT</td><td style="text-align:center;">Unknown</td><td style="text-align:center;">Commercial product</td></tr></tbody></table><p>We categorize each model by their type and also include their number of parameters, which represenmts the size of the model. Having more parameters allows the model to store more information. You can think of it like the size of the AI&#39;s brain: a bigger brain is smarter than a smaller one, but more expensive to maintain. You&#39;ll also notice that some LLMs are denoted as multilingual, meaning their creators chose to specifically give them more training data from languages other than English. That doesn&#39;t mean the other LLMs aren&#39;t trained on other languages, they just see much less of it and aren&#39;t optimized for handling more languages [1].</p><h3 id="evaluated-languages" tabindex="-1"><a class="header-anchor" href="#evaluated-languages" aria-hidden="true">#</a> Evaluated Languages</h3><img width="528" alt="Multilingual Translation Performance" src="https://github.com/wanchichen/mnlp_hw3/assets/29157715/e7b7d269-09ec-4fb2-99cb-724b16597410"><p>The above diagram shows the language families used in the evaluation. The paper considers 102 languages in 8 different language families. The number of languages in each bucket varies from 2 (Atlantic-Congo) to 13 (Other). The comprehensive list of languages can be found in Table 8 in the Appendix of the paper. Except for the experiment in Table 2, the paper considers English-centric translation meaning that either source or target language is English.</p><h2 id="llms-vs-dedicated-mt-models" tabindex="-1"><a class="header-anchor" href="#llms-vs-dedicated-mt-models" aria-hidden="true">#</a> LLMs vs Dedicated MT Models</h2><p>The main research question of the paper is to investigate whether LLMs are good enough translators. The authors have evaluated the various models extensively on diverse languages and presented the characteristics of the LLMs as a translator in various aspects. Here are the main findings:</p><ol><li><strong>The multilingual translation capabilities of LLMs are continually improving</strong>. The extensive evaluation of models revealed that more recent models like LLaMA2-7B perform better than the previously released LLMs. In particular, it was GPT-4 which is the most up-to-date model that achieved the highest average score in most directions in terms of BLEU and COMET. This suggests that it is possible to enhance the translation quality of these LLMs without using explicit parallel data.</li><li><strong>LLM\u2019s capability is unbalanced across languages</strong>. Although the model shows surprisingly good translation quality, it should be noted that their capabilities shine when the model is asked to translate sentences into English rather than translating from English. Additionally, the performance in other languages seems to be correlated with linguistic similarities with English. For example, the model showed much more impressive results in German than in Sino-Tibetan languages.</li><li><strong>LLMs still lag behind the strong supervised baseline, especially in low-resource languages</strong>. Although it is surprising that the LLMs perform reasonably well without supervised training, it is not competitive with the tailored MT systems yet. Concretely, NLLB, which is a model that was specifically trained for translation outperformed GPT-4 in more than 40% of the translation directions. When compared to the commercial system which has dedicated pre/post-processing steps on top of the neural models, the LLMs underperformed in most language directions as depicted in Figure 1.</li></ol><h2 id="factors-that-influence-an-llm-s-translation-performance" tabindex="-1"><a class="header-anchor" href="#factors-that-influence-an-llm-s-translation-performance" aria-hidden="true">#</a> Factors that Influence an LLM&#39;s Translation Performance</h2><ol><li><strong>LLMs can acquire translation ability in a resource-efficient way.</strong> Thanks to ICL, LLMs can learn to translate completely unseen languages, which allows them to translate new languages without additional training. The below plots compares the performance of the XGLM LLM on different languages with the size of the training data for each language. The higher the red bar, the better the model is at translating. The higher the blue bar, the more training data the model had for that language. The model does not need much data to generate good translations for many languages!</li></ol><img width="889" alt="image" src="https://github.com/wanchichen/mnlp_hw3/assets/39677488/88b527bc-b112-417d-9152-ff7f4b18e343"><ol start="2"><li><p><strong>Good performance requires a carefully-designed template.</strong> For LLMs to achieve good performance, ICL most be done properly. Using a wrong ICL template can reduce performance by 16% absoulute! What consitutes a good or bad ICL template though? Unfortunately, that can only be known by testing out each template. For example, the authors found that the common template of <code>[SRC]: &lt;X&gt; \\n [TGT]: &lt;Y&gt;</code> performed extremely poorly.</p></li><li><p><strong>Even an unreasonable template can instruct LLM to generate decent translation.</strong> Suprisingly, the authors of the paper found that giving the LLM the wrong instructions can still lead to reasonable performance. For example, they tried telling the LLM that <code>&lt;X&gt; can be summarized as &lt;Y&gt;</code> instead of <code>&lt;X&gt; can be translated as &lt;Y&gt;</code>. Even then, the LLM achieves near identical performance, and even improves on certain language pairs.</p></li><li><p><strong>Cross-lingual exemplars help for certain translation directions.</strong> When you have few examples for a specific translation direction, you can also boost performance by using examples from other languages during ICL. The authors found that showing the model Russian to English translations helped boost performance on the low resource Chinese to English. However, this doesn&#39;t always work. Performance can actually be degraded if your target translation pair is sufficiently resourced, like German to English translation.</p></li><li><p><strong>Semantically-related exemplars does not brings more benefits than randomly-picked exemplars.</strong> Typically, the performance gains from ICL are directly related to the quality and the relevance of the given examples. The authors tried using semantically similar sentences as the examples for ICL, but found that it actually performed <em>worse</em> than using randomly-picked examples. The authors hypothesize that while these examples may still be useful in helping the model learn the task of translation, the lack of diversity leads to poor translation knowledge.</p></li><li><p><strong>Exemplars teach the LLM the core feature of translation tasks.</strong> LLMs need correct examples to perform ICL. The authors tried using incorrect translation pairs as the examples, but that causes the LLM to completely fail at translating (leading a score of zero!). This indicates that the LLM does not inherently learn the task of translation during its pre-training stage. Duplicate examples also reduce the model&#39;s performance, further suggesting the importance of diversity.</p></li><li><p><strong>The exemplar in the tail of the prompt has more impact on the LLM\u2019s behaviour.</strong> The authors experimented with corrupting different examples given to the model, and found that corruptions in later examples had larger impacts on the model&#39;s translation quality. This is important for anyone using LLMs to translate, as they will need to consider the order of the translations fed to the model.</p></li></ol><h2 id="how-to-use-in-context-learning-on-your-own-data" tabindex="-1"><a class="header-anchor" href="#how-to-use-in-context-learning-on-your-own-data" aria-hidden="true">#</a> How to use In-Context Learning on your own Data</h2><p>Thanks to the availability of models like ChatGPT, the experiments of these papers can easily be applied to new datasets. We first do a comparison between ChatGPT and GPT4 via a qualitative evaluation for English to French as an example of high-resource language. Then, we extend the authors&#39; study on the effects of ICL to the new Quechua to Spanish translation pair to show the models performance on a low-resource language.</p><p>For French, we tried both zero-shot and 5-shot settings. For the 5-shot setting we randomly picked 5 French sentences from WMT 2014 dataset and used the following format for the prompt.</p><div class="language-text ext-text line-numbers-mode"><pre class="language-text"><code>Here are some parallel sentences between English and French:

(5 examples)

Now can you help me translate the following sentence?
(Sentence to translate to French)
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>In both zero-shot and few-shot settings, the model produced a high quality translation like below:</p><div class="language-text ext-text line-numbers-mode"><pre class="language-text"><code># source sentence
Mr. Hawkins stressed that a soldier could be prepared for his departure, with planning and consultation sessions, but that this was totally different than wanting to leave the army.
# reference target
M. Hawkins a soulign\xE9 qu&#39;un soldat pouvait \xEAtre pr\xE9par\xE9 \xE0 partir, avec des plans et des s\xE9ances de consultation, mais que cela \xE9tait totalement diff\xE9rent que le fait de vouloir quitter les rangs.

# Chat-GPT zero-shot
M. Hawkins a soulign\xE9 qu&#39;un soldat pourrait \xEAtre pr\xE9par\xE9 \xE0 son d\xE9part avec des sessions de planification et de consultation, mais que cela \xE9tait totalement diff\xE9rent du d\xE9sir de quitter l&#39;arm\xE9e.

# Chat-GPT few-shot
M. Hawkins a soulign\xE9 qu&#39;un soldat pouvait \xEAtre pr\xE9par\xE9 \xE0 son d\xE9part, avec des sessions de planification et de consultation, mais que cela \xE9tait totalement diff\xE9rent du d\xE9sir de quitter l&#39;arm\xE9e.

# GPT-4 zero-shot
M. Hawkins a soulign\xE9 qu&#39;un soldat pouvait \xEAtre pr\xE9par\xE9 \xE0 son d\xE9part, avec des s\xE9ances de planification et de consultation, mais que cela \xE9tait totalement diff\xE9rent de vouloir quitter l&#39;arm\xE9e.

# GPT-4 few-shot
M. Hawkins a soulign\xE9 qu&#39;un soldat pourrait \xEAtre pr\xE9par\xE9 \xE0 son d\xE9part avec des sessions de planification et de consultation, mais que cela \xE9tait totalement diff\xE9rent du d\xE9sir de quitter l&#39;arm\xE9e.
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>The first observation from the Chat-GPT&#39;s outputs are that the translations in both settings are both very accurate. In fact, the only difference between the zero-shot translation and the few-shot result was the minor change in conjugation (&#39;pouvait \xEAtre&#39; -&gt; &#39;pourrait \xEAtre&#39;) and the sentence were identical to each other in terms of the contents.</p><p>The result for GPT-4 is very similar. Since zero-shot result was already faithful to the source English sentence, we did not see significant quality improvement when adding examples that the model can learn in-context. The zero-shot output used the word &#39;s\xE9ances&#39; whereas the few-shot model used &#39;sessions&#39;, but they are synonyms.</p><h3 id="quechua-to-spanish" tabindex="-1"><a class="header-anchor" href="#quechua-to-spanish" aria-hidden="true">#</a> Quechua to Spanish</h3><p>We obtain data for Quechua to Spanish from Ortega et al. [2], which contains cleaned parallel sentences taken from the Bible. We use the test set for our evaluation and randomly take 5 sentences from the development set for the ICL examples.</p><table><thead><tr><th style="text-align:center;">Model</th><th style="text-align:center;">Type</th><th style="text-align:center;">BLEU</th></tr></thead><tbody><tr><td style="text-align:center;">ChatGPT</td><td style="text-align:center;">LLM</td><td style="text-align:center;">8.9</td></tr><tr><td style="text-align:center;">ChatGPT</td><td style="text-align:center;">LLM + ICL</td><td style="text-align:center;">17.4</td></tr><tr><td style="text-align:center;">Google Translate</td><td style="text-align:center;">MT</td><td style="text-align:center;">19.9</td></tr><tr><td style="text-align:center;">Ortega et al. [2]</td><td style="text-align:center;">MT</td><td style="text-align:center;">22.9</td></tr><tr><td style="text-align:center;">Chen and Fazio [3]</td><td style="text-align:center;">MT</td><td style="text-align:center;">23.7</td></tr></tbody></table><p>We find that the LLM&#39;s out-of-the-box performance is quite poor, obtaining only 8.9 BLEU (out of 100 total). Using ICL, however, significanlty improves the results to 17.4, almost performing as well as Google Translate (a multilingual MT model). However, neither result outperforms dedicated bilingual MT models [2,3] trained on in-domain data. That being said, the LLM&#39;s performance on Quechua-Spanish is already very impressive, and further improvements can likely be obtained with better ICL strategies.</p><h2 id="conclusion" tabindex="-1"><a class="header-anchor" href="#conclusion" aria-hidden="true">#</a> Conclusion</h2><h2 id="references" tabindex="-1"><a class="header-anchor" href="#references" aria-hidden="true">#</a> References</h2><p>[1] Briakou, Eleftheria, Colin Cherry, and George Foster. &quot;Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in PaLM&#39;s Translation Capability.&quot; arXiv preprint arXiv:2305.10266 (2023).</p><p>[2] Ortega, John E., Richard Castro Mamani, and Kyunghyun Cho. &quot;Neural machine translation with a polysynthetic low resource language.&quot; Machine Translation 34, no. 4 (2020): 325-346.</p><p>[3] Chen, William, and Brett Fazio. &quot;Morphologically-guided segmentation for translation of agglutinative low-resource languages.&quot; In Proceedings of the 4th Workshop on Technologies for MT of Low Resource Languages (LoResMT2021), pp. 20-31. 2021.</p>`,40);function k(C,_){const a=d("ExternalLinkIcon");return i(),r("div",null,[c,e("p",null,[u,g,m,e("a",p,[f,n(a)]),y]),o(" more "),e("p",null,[v,e("a",b,[x,n(a)])]),e("p",null,[L,e("a",w,[M,n(a)])]),T])}const I=s(h,[["render",k],["__file","index.html.vue"]]);export{I as default};
