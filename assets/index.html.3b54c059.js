const e=JSON.parse('{"key":"v-8eab3c46","path":"/dl4mt/2021/lass/","title":"Exploiting Capacity for Multilingual Neural Machine Translation","lang":"en-US","frontmatter":{"title":"Exploiting Capacity for Multilingual Neural Machine Translation","author":"Wenda Xu","date":"2021-11-19T00:00:00.000Z","tag":["Multilingual MT","Model Capacity","Language-specific Sub-network"],"category":["MT","DL4MT"],"summary":"Multiligual machine translation aims at learning a single tanslation model for multiple languages. However, high resource language often suffers from performance degradation.\\nIn this blog, we present a  method  LaSS proposed in a recent ACL paper on multilingual neural machine translation.\\nThe LaSS is an approach to jointly train a single unified multilingual MT model and learns language-specific subnetwork for each language pair. Authors conducted experiments on IWSLT and WMT datasets with various Transformer architectures. The experimental results demonstrates average 1.2 BLEU improvements on 36 language pairs. LaSS shows strong generalization capabilty and demonstrates strong performance in zero-shot translation. Specifically, LaSS achieves 8.3 BLEU on 30 language pairs.\\n","head":[["meta",{"property":"og:url","content":"https://lileicc.github.io/blog/dl4mt/2021/lass/"}],["meta",{"property":"og:site_name","content":"MLNLP Blog"}],["meta",{"property":"og:title","content":"Exploiting Capacity for Multilingual Neural Machine Translation"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://lileicc.github.io/blog/"}],["meta",{"property":"og:updated_time","content":"2022-09-13T03:45:15.000Z"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["meta",{"name":"twitter:image:alt","content":"Exploiting Capacity for Multilingual Neural Machine Translation"}],["meta",{"property":"article:author","content":"Wenda Xu"}],["meta",{"property":"article:tag","content":"Multilingual MT"}],["meta",{"property":"article:tag","content":"Model Capacity"}],["meta",{"property":"article:tag","content":"Language-specific Sub-network"}],["meta",{"property":"article:published_time","content":"2021-11-19T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2022-09-13T03:45:15.000Z"}]]},"excerpt":"<p>Multiligual machine translation aims at learning a single tanslation model for multiple languages. However, high resource language often suffers from performance degradation.\\nIn this blog, we present a  method  LaSS proposed in a recent ACL paper on multilingual neural machine translation.\\nThe LaSS is an approach to jointly train a single unified multilingual MT model and learns language-specific subnetwork for each language pair. Authors conducted experiments on IWSLT and WMT datasets with various Transformer architectures. The experimental results demonstrates average 1.2 BLEU improvements on 36 language pairs. LaSS shows strong generalization capabilty and demonstrates strong performance in zero-shot translation. Specifically, LaSS achieves 8.3 BLEU on 30 language pairs.</p>\\n","headers":[],"git":{"createdTime":1663040715000,"updatedTime":1663040715000,"contributors":[{"name":"Lei Li","email":"lileicc@gmail.com","commits":1}]},"readingTime":{"minutes":4.34,"words":1302},"filePathRelative":"dl4mt/2021/lass/README.md","localizedDate":"November 19, 2021"}');export{e as data};
