import{_ as r}from"./_plugin-vue_export-helper.cdc0426e.js";import{o as i,c as s,b as t,d as n,a as o,e,f as l,r as d}from"./app.594b312f.js";const c={},h=e("Large Language Models (LLMs) like "),g=t("a",{href:"chat.openai.com"},"ChatGPT",-1),u=e(" have rocked the world over the past year. They are able to perform almost any task you can think of, such as summarization, translation, and story-telling. But how do these LLMs work? And should you use them over existing tools? A "),m={href:"https://arxiv.org/pdf/2304.04675.pdf",target:"_blank",rel:"noopener noreferrer"},p=e("recent study"),f=e(" by AI researchers investigated how LLMs can be used for translation, and evaluate them against the best dedicated translation systems available. Our blog post today will summarize their findings and show how their study can be extended to new languages."),y=e("Paper: "),x={href:"https://arxiv.org/abs/2304.04675",target:"_blank",rel:"noopener noreferrer"},L=e("https://arxiv.org/abs/2304.04675"),b=e("Code: "),M={href:"https://github.com/NJUNLP/MMT-LLM",target:"_blank",rel:"noopener noreferrer"},v=e("https://github.com/NJUNLP/MMT-LLM"),_=l(`<h2 id="introduction" tabindex="-1"><a class="header-anchor" href="#introduction" aria-hidden="true">#</a> Introduction</h2><h2 id="in-context-learning" tabindex="-1"><a class="header-anchor" href="#in-context-learning" aria-hidden="true">#</a> In-Context Learning</h2><p>Large-scale machine learning models are trained on an extremely vast amount of data. But when it comes down to actual usage, we typically only need these models to perform certain tasks in certain domains. Thus, it is common to take a model pre-trained on a large general dataset and fine-tune it on a smaller task/domain specific dataset. However, this process is still expensive, as the amount of data necessary to achieve good performance on specific tasks after fine-tuning is still rather large, and the fine-tuning process itself requires access to GPU compute. Can we instead teach a model to perform a task without fine-tuning?</p><p>Recent research has shown that we can do achieve this with, in-context learning (ICL): the concept of showing an LLM a few examples of a task, before asking it to complete the task for a new data. In the context of translation, for example, this can be done by giving the LLM a few example translations, such as:</p><div class="language-text ext-text line-numbers-mode"><pre class="language-text"><code>I love potatoes -&gt; J&#39;aime les pommes de terre
Where did you buy that purse? -&gt; O\xF9 as-tu achet\xE9 ce sac \xE0 main?
You can feed deer at Nara Park -&gt; Vous pouvez nourrir les cerfs au parc de Nara
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Before asking it giving it this task:</p><div class="language-text ext-text line-numbers-mode"><pre class="language-text"><code>Let\u2019s see if its value is mentioned in any other responses. -&gt;
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>As you can probably guess, the examples you give the LLM can significantly affect its ability to perform ICL. The examples need to be represent the different possible ways to properly perform the task. As such, you may need to perform significant amounts of engineering to properly teach the LLM more difficult tasks, such as translation with rare languages or uncommon language pairs. ICL is possible in LLMs due to the nature of their pre-training task: predicting the next word. The examples provided by the user for ICL becomes the context the LLM uses to reatedly predict the next word, evenutally forming an output translation. This makes ICL a property mostly unique to LLMs, as it requires a specific pre-training type, along with a sufficiently large model/dataset.</p><h2 id="llms-vs-dedicated-mt-models" tabindex="-1"><a class="header-anchor" href="#llms-vs-dedicated-mt-models" aria-hidden="true">#</a> LLMs vs Dedicated MT Models</h2><h3 id="evaluated-models" tabindex="-1"><a class="header-anchor" href="#evaluated-models" aria-hidden="true">#</a> Evaluated Models</h3><p>The study compared eight LLMs (XGLM-7.5B, OPT-175B, Falcon-7B, BLOOMZ-7.1B, LLAMA2-7B, LLAMA2-7B-chat, ChatGPT, and GPT-4) with three dedicated multilingual machine translation (MT)models (M2M-100-12B, NLLB-1.3B, and Google Translate). A comparison is shown in the following table:</p><table><thead><tr><th style="text-align:center;">Model</th><th style="text-align:center;">Type</th><th style="text-align:center;">Parameters</th><th style="text-align:center;">Notes</th></tr></thead><tbody><tr><td style="text-align:center;">XGLM</td><td style="text-align:center;">LLM</td><td style="text-align:center;">7.5B</td><td style="text-align:center;">Multilingual</td></tr><tr><td style="text-align:center;">OPT</td><td style="text-align:center;">LLM</td><td style="text-align:center;">175B</td><td style="text-align:center;"></td></tr><tr><td style="text-align:center;">Falcon</td><td style="text-align:center;">LLM</td><td style="text-align:center;">7B</td><td style="text-align:center;"></td></tr><tr><td style="text-align:center;">BLOOMZ</td><td style="text-align:center;">LLM</td><td style="text-align:center;">7.1B</td><td style="text-align:center;">Multilingual</td></tr><tr><td style="text-align:center;">LLaMA2</td><td style="text-align:center;">LLM</td><td style="text-align:center;">7B</td><td style="text-align:center;"></td></tr><tr><td style="text-align:center;">LLaMA2-chat</td><td style="text-align:center;">LLM</td><td style="text-align:center;">7B</td><td style="text-align:center;">Trained for chatting</td></tr><tr><td style="text-align:center;">ChatGPT</td><td style="text-align:center;">LLM</td><td style="text-align:center;">175B</td><td style="text-align:center;">Trained for chatting</td></tr><tr><td style="text-align:center;">GPT-4</td><td style="text-align:center;">LLM</td><td style="text-align:center;">Unknown</td><td style="text-align:center;">Commercial product, multimodal, trained for chatting</td></tr><tr><td style="text-align:center;">M2M-100</td><td style="text-align:center;">MT</td><td style="text-align:center;">12B</td><td style="text-align:center;">Trained on 100 languages</td></tr><tr><td style="text-align:center;">NLLB</td><td style="text-align:center;">MT</td><td style="text-align:center;">1.3B</td><td style="text-align:center;">Trained on 200 languages</td></tr><tr><td style="text-align:center;">Google Translate</td><td style="text-align:center;">MT</td><td style="text-align:center;">Unknown</td><td style="text-align:center;">Commercial product</td></tr></tbody></table><p>We categorize each model by their type and also include their number of parameters, which represenmts the size of the model. Having more parameters allows the model to store more information. You can think of it like the size of the AI&#39;s brain: a bigger brain is smarter than a smaller one, but more expensive to maintain. You&#39;ll also notice that some LLMs are denoted as multilingual, meaning their creators chose to specifically give them more training data from languages other than English. That doesn&#39;t mean the other LLMs aren&#39;t trained on other languages, they just see much less of it and aren&#39;t optimized for handling more languages [1].</p><h2 id="factors-that-influence-an-llm-s-translation-performance" tabindex="-1"><a class="header-anchor" href="#factors-that-influence-an-llm-s-translation-performance" aria-hidden="true">#</a> Factors that Influence an LLM&#39;s Translation Performance</h2><ol><li>LLMs can acquire translation ability in a resource-efficient way.</li><li>Good performance requires a carefully-designed template</li></ol><h2 id="how-to-use-in-context-learning-on-your-own-data" tabindex="-1"><a class="header-anchor" href="#how-to-use-in-context-learning-on-your-own-data" aria-hidden="true">#</a> How to use In-Context Learning on your own Data</h2><h2 id="conclusion" tabindex="-1"><a class="header-anchor" href="#conclusion" aria-hidden="true">#</a> Conclusion</h2><h2 id="references" tabindex="-1"><a class="header-anchor" href="#references" aria-hidden="true">#</a> References</h2><p>[1] Briakou, Eleftheria, Colin Cherry, and George Foster. &quot;Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in PaLM&#39;s Translation Capability.&quot; arXiv preprint arXiv:2305.10266 (2023).</p>`,19);function w(k,T){const a=d("ExternalLinkIcon");return i(),s("div",null,[t("p",null,[h,g,u,t("a",m,[p,n(a)]),f]),o(" more "),t("p",null,[y,t("a",x,[L,n(a)])]),t("p",null,[b,t("a",M,[v,n(a)])]),_])}const I=r(c,[["render",w],["__file","index.html.vue"]]);export{I as default};
