const e=JSON.parse('{"key":"v-5b1d4715","path":"/dl4mt/2021/recurrent-attention/","title":"Recurrent Attention for Neural Machine Translation","lang":"en-US","frontmatter":{"title":"Recurrent Attention for Neural Machine Translation","author":"Jiachen Li","date":"2021-11-29T00:00:00.000Z","tag":["Transformer","Recurrent Attention"],"category":["MT","DL4MT"],"summary":"\u200BUpon its emergence, the Transformer Neural Networks [1] dominates the sequence-to-sequence tasks. It even outperforms the Google Neural Machine Translation model in specific tasks. Specifically, the multi-head attention mechanism that depends on element-wise dot-product is deemed as one of the critical building blocks to get things to work. But is it really that important?\\n","head":[["meta",{"property":"og:url","content":"https://lileicc.github.io/blog/dl4mt/2021/recurrent-attention/"}],["meta",{"property":"og:site_name","content":"MLNLP Blog"}],["meta",{"property":"og:title","content":"Recurrent Attention for Neural Machine Translation"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://lileicc.github.io/blog/"}],["meta",{"property":"og:updated_time","content":"2022-09-13T03:45:15.000Z"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["meta",{"name":"twitter:image:alt","content":"Recurrent Attention for Neural Machine Translation"}],["meta",{"property":"article:author","content":"Jiachen Li"}],["meta",{"property":"article:tag","content":"Transformer"}],["meta",{"property":"article:tag","content":"Recurrent Attention"}],["meta",{"property":"article:published_time","content":"2021-11-29T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2022-09-13T03:45:15.000Z"}]]},"excerpt":"<p>\u200BUpon its emergence, the Transformer Neural Networks [1] dominates the sequence-to-sequence tasks. It even outperforms the Google Neural Machine Translation model in specific tasks. Specifically, the multi-head attention mechanism that depends on element-wise dot-product is deemed as one of the critical building blocks to get things to work. But is it really that important?</p>\\n","headers":[{"level":2,"title":"Introduction","slug":"introduction","link":"#introduction","children":[]},{"level":2,"title":"Multi-head Attention Module","slug":"multi-head-attention-module","link":"#multi-head-attention-module","children":[]},{"level":2,"title":"Problem Associated with the Self-attention","slug":"problem-associated-with-the-self-attention","link":"#problem-associated-with-the-self-attention","children":[]},{"level":2,"title":"RAN: Recurrent Attention","slug":"ran-recurrent-attention","link":"#ran-recurrent-attention","children":[]},{"level":2,"title":"Effectiveness and Analysis of the RAN.","slug":"effectiveness-and-analysis-of-the-ran","link":"#effectiveness-and-analysis-of-the-ran","children":[{"level":3,"title":"1. Main results","slug":"_1-main-results","link":"#_1-main-results","children":[]},{"level":3,"title":"2. Analysis","slug":"_2-analysis","link":"#_2-analysis","children":[]}]},{"level":2,"title":"Summary","slug":"summary","link":"#summary","children":[]},{"level":2,"title":"References","slug":"references","link":"#references","children":[]}],"git":{"createdTime":1663040715000,"updatedTime":1663040715000,"contributors":[{"name":"Lei Li","email":"lileicc@gmail.com","commits":1}]},"readingTime":{"minutes":5.23,"words":1570},"filePathRelative":"dl4mt/2021/recurrent-attention/README.md","localizedDate":"November 29, 2021"}');export{e as data};
