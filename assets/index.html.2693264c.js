const e=JSON.parse('{"key":"v-5ea8eb91","path":"/dl4mt/2021/mgnmt/","title":"Mirror-Generative Neural Machine Translation","lang":"en-US","frontmatter":{"title":"Mirror-Generative Neural Machine Translation","author":"Tsu-Jui Fu","date":"2021-11-25T00:00:00.000Z","tag":["Variational Inference","Latent Variable Model","Semi-supervised Learning"],"category":["MT","DL4MT"],"summary":"In general, neural machine translation (NMT) requires a large amount of parallel data (e.g., EN-&gt;CN). However, it is not easy to collect enough high-quality parallelly-paired sentences for training the translation model. On the other hand, we can capture enormous plain text from Wikipedia or news articles for each specific language. In this paper, MGNMT tries to make good use of non-parallel data and boost the performance of NMT.\\n","head":[["meta",{"property":"og:url","content":"https://lileicc.github.io/blog/dl4mt/2021/mgnmt/"}],["meta",{"property":"og:site_name","content":"MLNLP Blog"}],["meta",{"property":"og:title","content":"Mirror-Generative Neural Machine Translation"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:updated_time","content":"2022-09-13T03:45:15.000Z"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"article:author","content":"Tsu-Jui Fu"}],["meta",{"property":"article:tag","content":"Variational Inference"}],["meta",{"property":"article:tag","content":"Latent Variable Model"}],["meta",{"property":"article:tag","content":"Semi-supervised Learning"}],["meta",{"property":"article:published_time","content":"2021-11-25T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2022-09-13T03:45:15.000Z"}]]},"excerpt":"<p>In general, neural machine translation (NMT) requires a large amount of parallel data (e.g., EN-&gt;CN). However, it is not easy to collect enough high-quality parallelly-paired sentences for training the translation model. On the other hand, we can capture enormous plain text from Wikipedia or news articles for each specific language. In this paper, MGNMT tries to make good use of non-parallel data and boost the performance of NMT.</p>\\n","headers":[{"level":2,"title":"Background (Back Translation)","slug":"background-back-translation","link":"#background-back-translation","children":[]},{"level":2,"title":"MGNMT","slug":"mgnmt","link":"#mgnmt","children":[{"level":3,"title":"Overview","slug":"overview","link":"#overview","children":[]},{"level":3,"title":"Parallel Training","slug":"parallel-training","link":"#parallel-training","children":[]},{"level":3,"title":"Non-parallel Training","slug":"non-parallel-training","link":"#non-parallel-training","children":[]},{"level":3,"title":"Decoding","slug":"decoding","link":"#decoding","children":[]}]},{"level":2,"title":"Exeperiments","slug":"exeperiments","link":"#exeperiments","children":[{"level":3,"title":"Dataset","slug":"dataset","link":"#dataset","children":[]},{"level":3,"title":"Quantitative Results","slug":"quantitative-results","link":"#quantitative-results","children":[]},{"level":3,"title":"Ablation Study","slug":"ablation-study","link":"#ablation-study","children":[]},{"level":3,"title":"Qualitative Examples","slug":"qualitative-examples","link":"#qualitative-examples","children":[]}]},{"level":2,"title":"Conclusion","slug":"conclusion","link":"#conclusion","children":[]},{"level":2,"title":"Reference","slug":"reference","link":"#reference","children":[]}],"git":{"createdTime":1663040715000,"updatedTime":1663040715000,"contributors":[{"name":"Lei Li","email":"lileicc@gmail.com","commits":1}]},"readingTime":{"minutes":4.26,"words":1277},"filePathRelative":"dl4mt/2021/mgnmt/README.md","localizedDate":"November 25, 2021"}');export{e as data};
