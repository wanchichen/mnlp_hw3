export const data = JSON.parse("{\"key\":\"v-5b1d4715\",\"path\":\"/dl4mt/2021/recurrent-attention/\",\"title\":\"Recurrent Attention for Neural Machine Translation\",\"lang\":\"en-US\",\"frontmatter\":{\"title\":\"Recurrent Attention for Neural Machine Translation\",\"author\":\"Jiachen Li\",\"date\":\"2021-11-29T00:00:00.000Z\",\"tag\":[\"Transformer\",\"Recurrent Attention\"],\"category\":[\"MT\",\"DL4MT\"],\"summary\":\"​Upon its emergence, the Transformer Neural Networks [1] dominates the sequence-to-sequence tasks. It even outperforms the Google Neural Machine Translation model in specific tasks. Specifically, the multi-head attention mechanism that depends on element-wise dot-product is deemed as one of the critical building blocks to get things to work. But is it really that important?\\n\",\"head\":[[\"meta\",{\"property\":\"og:url\",\"content\":\"https://lileicc.github.io/blog/dl4mt/2021/recurrent-attention/\"}],[\"meta\",{\"property\":\"og:site_name\",\"content\":\"MLNLP Blog\"}],[\"meta\",{\"property\":\"og:title\",\"content\":\"Recurrent Attention for Neural Machine Translation\"}],[\"meta\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"meta\",{\"property\":\"og:image\",\"content\":\"https://lileicc.github.io/blog/\"}],[\"meta\",{\"property\":\"og:locale\",\"content\":\"en-US\"}],[\"meta\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"meta\",{\"name\":\"twitter:image:alt\",\"content\":\"Recurrent Attention for Neural Machine Translation\"}],[\"meta\",{\"property\":\"article:author\",\"content\":\"Jiachen Li\"}],[\"meta\",{\"property\":\"article:tag\",\"content\":\"Transformer\"}],[\"meta\",{\"property\":\"article:tag\",\"content\":\"Recurrent Attention\"}],[\"meta\",{\"property\":\"article:published_time\",\"content\":\"2021-11-29T00:00:00.000Z\"}]]},\"excerpt\":\"<p>​Upon its emergence, the Transformer Neural Networks [1] dominates the sequence-to-sequence tasks. It even outperforms the Google Neural Machine Translation model in specific tasks. Specifically, the multi-head attention mechanism that depends on element-wise dot-product is deemed as one of the critical building blocks to get things to work. But is it really that important?</p>\\n\",\"headers\":[{\"level\":2,\"title\":\"Introduction\",\"slug\":\"introduction\",\"link\":\"#introduction\",\"children\":[]},{\"level\":2,\"title\":\"Multi-head Attention Module\",\"slug\":\"multi-head-attention-module\",\"link\":\"#multi-head-attention-module\",\"children\":[]},{\"level\":2,\"title\":\"Problem Associated with the Self-attention\",\"slug\":\"problem-associated-with-the-self-attention\",\"link\":\"#problem-associated-with-the-self-attention\",\"children\":[]},{\"level\":2,\"title\":\"RAN: Recurrent Attention\",\"slug\":\"ran-recurrent-attention\",\"link\":\"#ran-recurrent-attention\",\"children\":[]},{\"level\":2,\"title\":\"Effectiveness and Analysis of the RAN.\",\"slug\":\"effectiveness-and-analysis-of-the-ran\",\"link\":\"#effectiveness-and-analysis-of-the-ran\",\"children\":[{\"level\":3,\"title\":\"1. Main results\",\"slug\":\"_1-main-results\",\"link\":\"#_1-main-results\",\"children\":[]},{\"level\":3,\"title\":\"2. Analysis\",\"slug\":\"_2-analysis\",\"link\":\"#_2-analysis\",\"children\":[]}]},{\"level\":2,\"title\":\"Summary\",\"slug\":\"summary\",\"link\":\"#summary\",\"children\":[]},{\"level\":2,\"title\":\"References\",\"slug\":\"references\",\"link\":\"#references\",\"children\":[]}],\"readingTime\":{\"minutes\":5.23,\"words\":1570},\"filePathRelative\":\"dl4mt/2021/recurrent-attention/README.md\",\"localizedDate\":\"November 28, 2021\"}")

if (import.meta.webpackHot) {
  import.meta.webpackHot.accept()
  if (__VUE_HMR_RUNTIME__.updatePageData) {
    __VUE_HMR_RUNTIME__.updatePageData(data)
  }
}

if (import.meta.hot) {
  import.meta.hot.accept(({ data }) => {
    __VUE_HMR_RUNTIME__.updatePageData(data)
  })
}
